{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae_ml_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NxP0ZNBvlhC"
      },
      "source": [
        "VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMJRTr1Z4mM9"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import functools\n",
        "import os\n",
        "\n",
        "# Dependency imports\n",
        "from absl import flags\n",
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "IMAGE_SHAPE = [28, 28, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a3Czf-NgzKR"
      },
      "source": [
        "\n",
        "from scipy.io import loadmat\n",
        "\n",
        "\n",
        "def load_omniglot(batch_size):\n",
        "    n_validation=1345\n",
        "    # set args\n",
        "    input_size = [1, 28, 28]\n",
        "    input_type = 'binary'\n",
        "    dynamic_binarization = True\n",
        "\n",
        "    # start processing\n",
        "    def reshape_data(data):\n",
        "        return data.reshape((-1, 28, 28)).reshape((-1, 28*28), order='F')\n",
        "    omni_raw = loadmat(os.path.join('/content', 'chardata.mat'))\n",
        "\n",
        "    # train and test data\n",
        "    train_data = reshape_data(omni_raw['data'].T.astype('float32'))\n",
        "    x_test = reshape_data(omni_raw['testdata'].T.astype('float32'))\n",
        "\n",
        "    # shuffle train data\n",
        "    np.random.shuffle(train_data)\n",
        "\n",
        "    # set train and validation data\n",
        "    x_train = train_data[:-n_validation]\n",
        "    x_val = train_data[-n_validation:]\n",
        "\n",
        "    # binarize\n",
        "    np.random.seed(777)\n",
        "    x_val = np.random.binomial(1, x_val)\n",
        "    x_test = np.random.binomial(1, x_test)\n",
        "\n",
        "    # idle y's\n",
        "    y_train = np.zeros( (x_train.shape[0], 1) )\n",
        "    y_val = np.zeros( (x_val.shape[0], 1) )\n",
        "    y_test = np.zeros( (x_test.shape[0], 1) )\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    def train():\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "      train_dataset.shuffle(50000).repeat().batch(batch_size)\n",
        "      return tf.compat.v1.data.make_one_shot_iterator(train_dataset).get_next()\n",
        "\n",
        "    def eval():\n",
        "      validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "      validation_dataset.batch(batch_size)\n",
        "      return tf.compat.v1.data.make_one_shot_iterator(validation_dataset).get_next()\n",
        "\n",
        "    #validation = data_utils.TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val))\n",
        "    #val_loader = data_utils.DataLoader(validation, batch_size=args.test_batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "    return train, eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv4bJp1D4wuy"
      },
      "source": [
        "def _softplus_inverse(x):\n",
        "  \"\"\"Helper which computes the function inverse of `tf.nn.softplus`.\"\"\"\n",
        "  return tf.math.log(tf.math.expm1(x))\n",
        "\n",
        "\n",
        "def make_encoder(activation, latent_size, base_depth):\n",
        "  \"\"\"Creates the encoder function.\n",
        "\n",
        "  Args:\n",
        "    activation: Activation function in hidden layers.\n",
        "    latent_size: The dimensionality of the encoding.\n",
        "    base_depth: The lowest depth for a layer.\n",
        "\n",
        "  Returns:\n",
        "    encoder: A `callable` mapping a `Tensor` of images to a\n",
        "      `tfd.Distribution` instance over encodings.\n",
        "  \"\"\"\n",
        "  # conv = functools.partial(\n",
        "  #     tf.keras.layers.Conv2D, padding=\"SAME\", activation=activation)\n",
        "\n",
        "  # encoder_net = tf.keras.Sequential([\n",
        "  #     conv(base_depth, 5, 1),\n",
        "  #     conv(base_depth, 5, 2),\n",
        "  #     conv(2 * base_depth, 5, 1),\n",
        "  #     conv(2 * base_depth, 5, 2),\n",
        "  #     conv(4 * latent_size, 7, padding=\"VALID\"),\n",
        "  #     tf.keras.layers.Flatten(),\n",
        "  #     tf.keras.layers.Dense(2 * latent_size, activation=None),\n",
        "  # ])\n",
        "  encoder_net = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(300, input_dim= 28*28),\n",
        "        tf.keras.layers.Dense(300, activation = tf.nn.silu),\n",
        "        tf.keras.layers.Dense(300, activation = tf.nn.silu),\n",
        "        tf.keras.layers.Dense(2 * latent_size, activation=None),\n",
        "    ])\n",
        "  def encoder(images):\n",
        "    images = 2 * tf.cast(images, dtype=tf.float32) - 1\n",
        "    net = encoder_net(images)\n",
        "    return tfd.MultivariateNormalDiag(\n",
        "        loc= net[..., :latent_size],\n",
        "        scale_diag=tf.nn.softplus(net[..., latent_size:] +\n",
        "                                  _softplus_inverse(1.0)),\n",
        "        name=\"code\")\n",
        "\n",
        "  return encoder\n",
        "\n",
        "\n",
        "def make_decoder(activation, latent_size, output_shape, base_depth):\n",
        "  \"\"\"Creates the decoder function.\n",
        "\n",
        "  Args:\n",
        "    activation: Activation function in hidden layers.\n",
        "    latent_size: Dimensionality of the encoding.\n",
        "    output_shape: The output image shape.\n",
        "    base_depth: Smallest depth for a layer.\n",
        "\n",
        "  Returns:\n",
        "    decoder: A `callable` mapping a `Tensor` of encodings to a\n",
        "      `tfd.Distribution` instance over images.\n",
        "  \"\"\"\n",
        "  # deconv = functools.partial(\n",
        "  #     tf.keras.layers.Conv2DTranspose, padding=\"SAME\", activation=activation)\n",
        "  # conv = functools.partial(\n",
        "  #     tf.keras.layers.Conv2D, padding=\"SAME\", activation=activation)\n",
        "\n",
        "  # decoder_net = tf.keras.Sequential([\n",
        "  #     deconv(2 * base_depth, 7, padding=\"VALID\"),\n",
        "  #     deconv(2 * base_depth, 5),\n",
        "  #     deconv(2 * base_depth, 5, 2),\n",
        "  #     deconv(base_depth, 5),\n",
        "  #     deconv(base_depth, 5, 2),\n",
        "  #     deconv(base_depth, 5),\n",
        "  #     conv(output_shape[-1], 5, activation=None),\n",
        "  # ])\n",
        "\n",
        "  decoder_net = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(300, input_dim= latent_size),\n",
        "        tf.keras.layers.Dense(300,  activation = tf.nn.silu),\n",
        "        tf.keras.layers.Dense(300,  activation = tf.nn.silu),\n",
        "        tf.keras.layers.Dense(28 * 28 , activation=None),\n",
        "    ])\n",
        "\n",
        "  def decoder(codes):\n",
        "    original_shape = tf.shape(input=codes)\n",
        "    # Collapse the sample and batch dimension and convert to rank-4 tensor for\n",
        "    # use with a convolutional decoder network.\n",
        "    codes = tf.reshape(codes, (-1, 1, 1, latent_size))\n",
        "    logits = decoder_net(codes)\n",
        "    logits = tf.reshape(\n",
        "        logits, shape=tf.concat([original_shape[:-1], output_shape], axis=0))\n",
        "    return tfd.Independent(tfd.Bernoulli(logits=logits),\n",
        "                           reinterpreted_batch_ndims=len(output_shape),\n",
        "                           name=\"image\")\n",
        "\n",
        "  return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMpXvDL0vjv-"
      },
      "source": [
        "def make_mixture_prior(latent_size, mixture_components):\n",
        "  \"\"\"Creates the mixture of Gaussians prior distribution.\n",
        "\n",
        "  Args:\n",
        "    latent_size: The dimensionality of the latent representation.\n",
        "    mixture_components: Number of elements of the mixture.\n",
        "\n",
        "  Returns:\n",
        "    random_prior: A `tfd.Distribution` instance representing the distribution\n",
        "      over encodings in the absence of any evidence.\n",
        "  \"\"\"\n",
        "  if mixture_components == 1:\n",
        "    # See the module docstring for why we don't learn the parameters here.\n",
        "    return tfd.MultivariateNormalDiag(\n",
        "        loc=tf.zeros([latent_size]),\n",
        "        scale_identity_multiplier=1.0)\n",
        "\n",
        "  loc = tf.compat.v1.get_variable(\n",
        "      name=\"loc\", shape=[mixture_components, latent_size])\n",
        "  raw_scale_diag = tf.compat.v1.get_variable(\n",
        "      name=\"raw_scale_diag\", shape=[mixture_components, latent_size])\n",
        "  mixture_logits = tf.compat.v1.get_variable(\n",
        "      name=\"mixture_logits\", shape=[mixture_components])\n",
        "\n",
        "  return tfd.MixtureSameFamily(\n",
        "      components_distribution=tfd.MultivariateNormalDiag(\n",
        "          loc=loc,\n",
        "          scale_diag=tf.nn.softplus(raw_scale_diag)),\n",
        "      mixture_distribution=tfd.Categorical(logits=mixture_logits),\n",
        "      name=\"prior\")\n",
        "\n",
        "\n",
        "def pack_images(images, rows, cols):\n",
        "  \"\"\"Helper utility to make a field of images.\"\"\"\n",
        "  shape = tf.shape(input=images)\n",
        "  width = shape[-3]\n",
        "  height = shape[-2]\n",
        "  depth = shape[-1]\n",
        "  images = tf.reshape(images, (-1, width, height, depth))\n",
        "  batch = tf.shape(input=images)[0]\n",
        "  rows = tf.minimum(rows, batch)\n",
        "  cols = tf.minimum(batch // rows, cols)\n",
        "  images = images[:rows * cols]\n",
        "  images = tf.reshape(images, (rows, cols, width, height, depth))\n",
        "  images = tf.transpose(a=images, perm=[0, 2, 1, 3, 4])\n",
        "  images = tf.reshape(images, [1, rows * width, cols * height, depth])\n",
        "  return images\n",
        "\n",
        "\n",
        "def image_tile_summary(name, tensor, rows=8, cols=8):\n",
        "  tf.compat.v1.summary.image(\n",
        "      name, pack_images(tensor, rows, cols), max_outputs=1)\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params, config):\n",
        "  \"\"\"Builds the model function for use in an estimator.\n",
        "\n",
        "  Args:\n",
        "    features: The input features for the estimator.\n",
        "    labels: The labels, unused here.\n",
        "    mode: Signifies whether it is train or test or predict.\n",
        "    params: Some hyperparameters as a dictionary.\n",
        "    config: The RunConfig, unused here.\n",
        "\n",
        "  Returns:\n",
        "    EstimatorSpec: A tf.estimator.EstimatorSpec instance.\n",
        "  \"\"\"\n",
        "  del labels, config\n",
        "\n",
        "  encoder = make_encoder(params[\"activation\"],\n",
        "                         params[\"latent_size\"],\n",
        "                         params[\"base_depth\"])\n",
        "  decoder = make_decoder(params[\"activation\"],\n",
        "                         params[\"latent_size\"],\n",
        "                         IMAGE_SHAPE,\n",
        "                         params[\"base_depth\"])\n",
        "  latent_prior = make_mixture_prior(params[\"latent_size\"],\n",
        "                                    params[\"mixture_components\"])\n",
        "\n",
        "  image_tile_summary(\n",
        "      \"input\", tf.cast(features, dtype=tf.float32), rows=1, cols=16)\n",
        "\n",
        "  approx_posterior = encoder(features)\n",
        "  approx_posterior_sample = approx_posterior.sample(params[\"n_samples\"])\n",
        "  decoder_likelihood = decoder(approx_posterior_sample)\n",
        "  image_tile_summary(\n",
        "      \"recon/sample\",\n",
        "      tf.cast(decoder_likelihood.sample()[:3, :16], dtype=tf.float32),\n",
        "      rows=3,\n",
        "      cols=16)\n",
        "  image_tile_summary(\n",
        "      \"recon/mean\",\n",
        "      decoder_likelihood.mean()[:3, :16],\n",
        "      rows=3,\n",
        "      cols=16)\n",
        "\n",
        "  # `distortion` is just the negative log likelihood.\n",
        "  distortion = -decoder_likelihood.log_prob(features)\n",
        "  avg_distortion = tf.reduce_mean(input_tensor=distortion)\n",
        "  tf.compat.v1.summary.scalar(\"distortion\", avg_distortion)\n",
        "  print(\"LOGLIKELIHOOD : \", -avg_distortion)\n",
        "\n",
        "  rate = (approx_posterior.log_prob(approx_posterior_sample)\n",
        "            - latent_prior.log_prob(approx_posterior_sample))\n",
        "  \n",
        "  avg_rate = tf.reduce_mean(input_tensor=rate)\n",
        "  tf.compat.v1.summary.scalar(\"rate\", avg_rate)\n",
        "\n",
        "  elbo_local = -(rate + distortion)\n",
        "\n",
        "  elbo = tf.reduce_mean(input_tensor=elbo_local)\n",
        "  loss = -elbo\n",
        "  tf.compat.v1.summary.scalar(\"elbo\", elbo)\n",
        "\n",
        "  importance_weighted_elbo = tf.reduce_mean(\n",
        "      input_tensor=tf.reduce_logsumexp(input_tensor=elbo_local, axis=0) -\n",
        "      tf.math.log(tf.cast(params[\"n_samples\"], dtype=tf.float32)))\n",
        "  tf.compat.v1.summary.scalar(\"elbo/importance_weighted\",\n",
        "                              importance_weighted_elbo)\n",
        "\n",
        "  # Decode samples from the prior for visualization.\n",
        "  random_image = decoder(latent_prior.sample(16))\n",
        "  image_tile_summary(\n",
        "      \"random/sample\",\n",
        "      tf.cast(random_image.sample(), dtype=tf.float32),\n",
        "      rows=4,\n",
        "      cols=4)\n",
        "  image_tile_summary(\"random/mean\", random_image.mean(), rows=4, cols=4)\n",
        "\n",
        "  # Perform variational inference by minimizing the -ELBO.\n",
        "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "  learning_rate = tf.compat.v1.train.cosine_decay(\n",
        "      params[\"learning_rate\"], global_step, params[\"max_steps\"])\n",
        "  tf.compat.v1.summary.scalar(\"learning_rate\", learning_rate)\n",
        "  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "      loss=loss,\n",
        "      train_op=train_op,\n",
        "      eval_metric_ops={\n",
        "          \"elbo\":\n",
        "              tf.compat.v1.metrics.mean(elbo),\n",
        "          \"elbo/importance_weighted\":\n",
        "              tf.compat.v1.metrics.mean(importance_weighted_elbo),\n",
        "          \"rate\":\n",
        "              tf.compat.v1.metrics.mean(avg_rate),\n",
        "          \"distortion\":\n",
        "              tf.compat.v1.metrics.mean(avg_distortion),\n",
        "      },\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EqhK09i5AZH"
      },
      "source": [
        "ROOT_PATH = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
        "FILE_TEMPLATE = \"binarized_mnist_{split}.amat\"\n",
        "\n",
        "# ROOT_PATH = \"https://people.cs.umass.edu/~marlin/data/\"\n",
        "# FILE_TEMPLATE = \"caltech101_silhouettes_28_split1.mat\"\n",
        "\n",
        "\n",
        "def download(directory, filename):\n",
        "  \"\"\"Downloads a file.\"\"\"\n",
        "  filepath = os.path.join(directory, filename)\n",
        "  if tf.io.gfile.exists(filepath):\n",
        "    return filepath\n",
        "  if not tf.io.gfile.exists(directory):\n",
        "    tf.io.gfile.makedirs(directory)\n",
        "  url = os.path.join(ROOT_PATH, filename)\n",
        "  print(\"Downloading %s to %s\" % (url, filepath))\n",
        "  urllib.request.urlretrieve(url, filepath)\n",
        "  return filepath\n",
        "\n",
        "\n",
        "def static_mnist_dataset(directory, split_name):\n",
        "  \"\"\"Returns binary static MNIST tf.data.Dataset.\"\"\"\n",
        "  amat_file = download(directory, FILE_TEMPLATE.format(split=split_name))\n",
        "  dataset = tf.data.TextLineDataset(amat_file)\n",
        "  str_to_arr = lambda string: np.array([c == b\"1\" for c in string.split()])\n",
        "\n",
        "  def _parser(s):\n",
        "    booltensor = tf.compat.v1.py_func(str_to_arr, [s], tf.bool)\n",
        "    reshaped = tf.reshape(booltensor, [28, 28, 1])\n",
        "    return tf.cast(reshaped, dtype=tf.float32), tf.constant(0, tf.int32)\n",
        "\n",
        "  return dataset.map(_parser)\n",
        "\n",
        "\n",
        "\n",
        "def build_input_fns(data_dir, batch_size):\n",
        "  \"\"\"Builds an Iterator switching between train and heldout data.\"\"\"\n",
        "\n",
        "  # Build an iterator over training batches.\n",
        "  def train_input_fn():\n",
        "    dataset = static_mnist_dataset(data_dir, \"train\")\n",
        "    dataset = dataset.shuffle(50000).repeat().batch(batch_size)\n",
        "    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
        "\n",
        "  # Build an iterator over the heldout set.\n",
        "  def eval_input_fn():\n",
        "    eval_dataset = static_mnist_dataset(data_dir, \"valid\")\n",
        "    eval_dataset = eval_dataset.batch(batch_size)\n",
        "    return tf.compat.v1.data.make_one_shot_iterator(eval_dataset).get_next()\n",
        "\n",
        "  return train_input_fn, eval_input_fn\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "  del argv  # unused\n",
        "\n",
        "  params = {}\n",
        "  params[\"learning_rate\"] = 0.001\n",
        "  params[\"max_steps\"] = 500\n",
        "  params[\"latent_size\"] = 16\n",
        "  params[\"base_depth\"] = 32\n",
        "  params[\"activation\"] = getattr(tf.nn, \"leaky_relu\")\n",
        "  params[\"batch_size\"] = 32\n",
        "  params[\"n_samples\"] = 16\n",
        "  params[\"mixture_components\"] = 10 #standard\n",
        "  params[\"viz_steps\"] = 4000\n",
        "\n",
        "  # model_dir = \"\\content\\Results\"\n",
        "  data_dir =os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"), \"vae/data\")\n",
        "\n",
        "  train_input_fn, eval_input_fn = build_input_fns(data_dir, params[\"batch_size\"])\n",
        "  #train_1, eval_1 = load_omniglot(16)\n",
        "\n",
        "  estimator = tf.estimator.Estimator(\n",
        "      model_fn,\n",
        "      params=params,\n",
        "      config=None\n",
        "  )\n",
        "\n",
        "  estimator = tf.estimator.Estimator(\n",
        "      model_fn,\n",
        "      params=params,\n",
        "      config=tf.estimator.RunConfig(\n",
        "          model_dir=\"/content/Results\",\n",
        "          save_checkpoints_steps=params[\"viz_steps\"],\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  # for _ in range(params[\"max_steps\"] // params[\"viz_steps\"]):\n",
        "  #   estimator.train(train_input_fn, steps=params[\"viz_steps\"])\n",
        "  #   eval_results = estimator.evaluate(eval_input_fn)\n",
        "  #   print(\"Evaluation_results:\\n\\t%s\\n \" % eval_results)\n",
        "\n",
        "  estimator.train(train_input_fn, steps=params[\"viz_steps\"])\n",
        "  print(\"##################### TRAINING FINISHED ############################\")\n",
        "  eval_results = estimator.evaluate(eval_input_fn)\n",
        "  print(\"Evaluation_results:\\n\\t%s\\n \" % eval_results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  tf.compat.v1.app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFEX9jTpJRSG"
      },
      "source": [
        "logdir = \"/content/Results/\"\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/Results/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}